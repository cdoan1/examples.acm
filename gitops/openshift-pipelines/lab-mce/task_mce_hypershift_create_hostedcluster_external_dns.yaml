---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: mce-hypershift-create-hostedcluster-external-dns
  namespace: open-cluster-management-pipelines-mce
spec:
  params:
  - name: kubeconfig
    default: ""
    description: "base64 encoded kubeconfig of the cluster we want to run this task on. Default to empty string, use the current cluster."
    type: string
  - name: managed-cluster
    default: "chc-extdns"
    description: "name of this managed cluster, will use this value to construct the hostedcluster name."
    type: string
  steps:
  - args:
    - |-
      set -e
      set -x
      
      # Reference: https://hypershift-docs.netlify.app/how-to/external-dns/

      TOP=$(pwd)
      PULL_SECRET=$TOP/pull-secret-hypershift.json
      export NS=open-cluster-management-pipelines-rhacm
      oc get secret multiclusterhub-operator-pull-secret-basic -n $NS -ojsonpath='{.data.\.dockerconfigjson}' | base64 --decode > $PULL_SECRET

      if [ -f $(pwd)/remote.kubeconfig ]; then
        export KUBECONFIG=$(pwd)/remote.kubeconfig
      else
        echo "Expected to find remote.kubeconfig. Existing."
        exit 1
      fi

      oc cluster-info
      
      OIDC_BUCKET_NAME=playbacknext-hypershift
      OIDC_BUCKET_REGION=us-west-1
      BASE_DOMAIN=demo.red-chesterfield.com
      HOSTED_CLUSTER_BASE_DOMAIN=stolostron.io
      CLUSTER_NAME=$(params.managed-cluster)-hostedcluster
      INFRA_ID=$CLUSTER_NAME
      REGION=us-west-1

      if [ ! -f $AWS_CREDS ]; then
        echo "Requires AWS credentials file."
        exit 1
      fi

      AWS_CREDS=credentials
      OUTPUT_INFRA_FILE=$TOP/$CLUSTER_NAME-hypershift_test_infra.json
      OUTPUT_IAM_FILE=$TOP/$CLUSTER_NAME-hypershift_test_iam.json

      # hypershift create cluster aws --name=example --endpoint-access=PublicAndPrivate --external-dns-domain=service.hypershift.example.org ...
      # hypershift create cluster aws \
      #     --aws-creds ${AWS_CREDS} \
      #     --instance-type m6i.xlarge \
      #     --region ${REGION} \
      #     --auto-repair \
      #     --generate-ssh \
      #     --name jparrill-hosted \
      #     --namespace clusters \
      #     --base-domain jparrill-dev-public.aws.kerbeross.com \
      #     --node-pool-replicas 2 \
      #     --pull-secret ${HOME}/pull_secret.json \
      #     --release-image quay.io/openshift-release-dev/ocp-release:4.12.0-ec.3-x86_64 \
      #     --external-dns-domain=jparrill-hosted-public.aws.kerbeross.com \
      #     --endpoint-access=PublicAndPrivate

      HYPERSHIFT=$(pwd)/hypershift/bin/hypershift.12
      RELEASE=quay.io/openshift-release-dev/ocp-release:4.12.11-x86_64

      echo "üñêÔ∏è generate hostedcluster $CLUSTER_NAME manifest ..."
      ${HYPERSHIFT} create cluster aws \
        --aws-creds $AWS_CREDS \
        --region $REGION \
        --auto-repair \
        --instance-type m6i.xlarge \
        --generate-ssh \
        --name $CLUSTER_NAME \
        --namespace local-cluster \
        --base-domain stolostron.io \
        --node-pool-replicas 3 \
        --pull-secret $PULL_SECRET \
        --external-dns-domain=cdoan.stolostron.io \
        --endpoint-access=Public \
        --render > $CLUSTER_NAME-hosted-cluster.yaml

        # --release-image quay.io/openshift-release-dev/ocp-release:4.12.0-ec.3-x86_64 \

      echo "üñêÔ∏è creating hostedcluster aws $CLUSTER_NAME"
      oc apply -f $CLUSTER_NAME-hosted-cluster.yaml
      oc annotate hostedcluster $CLUSTER_NAME "cluster.open-cluster-management.io/hypershiftdeployment=default/local-cluster" -n local-cluster --overwrite

      echo "üñêÔ∏è verify whoami ..." && oc whoami

      echo "üñêÔ∏è create managedcluster CR ..."
      cat <<EOF | oc apply -f -
      apiVersion: cluster.open-cluster-management.io/v1
      kind: ManagedCluster
      metadata:
        annotations:
          import.open-cluster-management.io/klusterlet-deploy-mode: Hosted
          import.open-cluster-management.io/hosting-cluster-name: local-cluster
          import.open-cluster-management.io/klusterlet-namespace: klusterlet-$INFRA_ID
          open-cluster-management/created-via: other
        labels:
          cloud: auto-detect
          cluster.open-cluster-management.io/clusterset: default
          name: $INFRA_ID
          vendor: OpenShift
        name: $INFRA_ID
      spec:
        hubAcceptsClient: true
        leaseDurationSeconds: 60
      EOF

      echo "üñêÔ∏è wait 10m for hostedcluster to be available ..."
      oc wait --for="condition=Available" hostedcluster $CLUSTER_NAME -n local-cluster --timeout=600s
      oc wait --for="condition=ReconciliationSucceeded" hostedcluster $CLUSTER_NAME -n local-cluster --timeout=600s

      oc apply -f - <<EOF
      ---
      apiVersion: addon.open-cluster-management.io/v1alpha1
      kind: ManagedClusterAddOn
      metadata:
        name: config-policy-controller
        namespace: $CLUSTER_NAME
      spec:
        installNamespace: open-cluster-management-agent-addon
      ---
      apiVersion: addon.open-cluster-management.io/v1alpha1
      kind: ManagedClusterAddOn
      metadata:
        name: governance-policy-framework
        namespace: $CLUSTER_NAME
      spec:
        installNamespace: open-cluster-management-agent-addon
      EOF

      echo "üñêÔ∏è wait 10m for hostedcluster ReconciliationSucceeded to be true ..."
      oc wait --for="condition=ReconciliationSucceeded" hostedcluster $INFRA_ID -n local-cluster --timeout=600s || true
      echo "üñêÔ∏è wait 10m for managedcluster to be available ..."
      oc wait --for="condition=ManagedClusterConditionAvailable" managedcluster $INFRA_ID --timeout=600s || true

      oc get hostedclusters -A
      oc get managedclusters -A
      oc get managedclusteraddon -A
      
      oc get mch multiclusterhub -n open-cluster-management -ojsonpath='{.status.currentVersion}'
      
      oc get mce multiclusterengine -n multicluster-engine -ojsonpath='{.status.currentVersion}'
      
      oc get pods -n local-cluster-$CLUSTER_NAME
      
      oc get pods -n klusterlet-$CLUSTER_NAME
      oc get pods -n klusterlet-$CLUSTER_NAME -o yaml | grep "image:" | sort -u | grep -v f:image | sed s'#    -#     #' | sort -u | sed 's/^[ \t]*//'
      
      if oc get secret -n local-cluster $CLUSTER_NAME-admin-kubeconfig; then
        oc get secret -n local-cluster $CLUSTER_NAME-admin-kubeconfig -ojsonpath='{.data.kubeconfig}' | base64 --decode > $CLUSTER_NAME-hostedcluster.kubeconfig
        if [ -f $CLUSTER_NAME-hostedcluster.kubeconfig ]; then
          export KUBECONFIG=$CLUSTER_NAME-hostedcluster.kubeconfig
          oc cluster-info
          oc get co
          oc get pods -n open-cluster-management-agent-addon
          oc get pods -n open-cluster-management-agent-addon -o yaml | grep "image:" | sort -u | grep -v f:image | sed s'#    -#     #' | sort -u | sed 's/^[ \t]*//'
        fi
      fi

      echo "Done with hostedcluster task ..."

      set +x
      set +e
      exit 0
    command:
    - /bin/bash
    - -c
    image: quay.io/acm-sre/ocm-utils:latest
    name: apply
    resources: {}
    workingDir: /workspace/source
  workspaces:
  - name: source
