{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Examples with ACM","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"ansible/","title":"Ansible","text":""},{"location":"ansible/#ansible-collection-stolostroncore","title":"ansible collection stolostron.core","text":"<p>If you have deployed Ansible Automation Platform in a cluster, you can setup an ansible dynamic inventory of all the managed clusters managed by ACM/MCE.</p> <p>Starting in RHACM 2.5, we added support for cluster-proxy and managed-serviceaccount capabilities in ACM/MCE on the hub and managed cluster side. When enabled, the ACM/MCE hub acts as a connection proxy to all the managed clusters, and AAP can manage those clusters through the ACM/MCE.</p>"},{"location":"ansible/#how-this-works","title":"How this works","text":"<ol> <li>Add a kubeconfig credential to AAP to allow access to the cluster-proxy route on the ACM/MCE hub.</li> <li>Setup the stolostron.core collection (client side code) on the AAP cluster. The ansible playbooks will use this module to connect to the ACM/MCE hub cluster, and handshake with the managed cluster. Exposeing the managed clusters to the anisible playbook.</li> </ol> <p>You can run this playbook to setup dyanmic inventory of all the OCM/ACM/MCE managed clusters. The ansible-collection.core is provided through an exeuction engine docker container.</p> <pre><code>ansible-playbook playbooks/aap-controller-ocm-collection-setup.yml -e tenant=playback\n</code></pre>"},{"location":"ansible/#use-the-dynamic-inventory-to-manage-new-provisioned-clusters-with-ansible-post-install-webhook","title":"Use the dynamic inventory to manage new provisioned clusters with Ansible post-install webhook","text":"<p>We have enabled integration across the 3 pillars in ACM--cluster lifecycle, policy, and application. Initially, the focus was to allow manaaging exsternal systems based on steps in the three pillars. There was no way to run playbooks against cluster that was just provisioned automaticially, end to end. Manual intervention was always required.</p> <p>With dyanmic inventory and the cluster-proxy we can achieve this. </p> <ol> <li>Create an ansible job template, to perform some management task on a new managed cluster.</li> <li>Create the corresponding job template automation reference in RHACM.</li> <li>When the cluster is provisioned, it will be imported into RHACM automatically.</li> <li>The post-install job template will trigger 4.1 The job template will sync the inventory to include the new managed cluster 4.2 The ansible job will be run, and include the extra_vars variable ansible_limit= 4.3 The job template should run against ansible host in question."},{"location":"ansible/#connecting-the-dots","title":"Connecting the dots","text":"<p>When we create a cluster with an ansible post install job through the CONSOLE, these are the related CR.</p> graph LR     NS:acmsre-cluster-proxy-23 --&gt; managedcluster     NS:acmsre-cluster-proxy-23 --&gt; clustercurator     NS:acmsre-cluster-proxy-23 --&gt; job ---&gt; acmsre-cluster-proxy-23-0-hg27h-provision     job --&gt; curator-job-pk256 --&gt; ansiblejob     job --&gt; posthookjob-w4bpv     ansiblejob ---&gt; posthookjob-gb6qd  <ul> <li>If we want to run a playbook against a specific managed cluster, the playback has to be written where the inventory host needs to be passed as a variable to the playbook.</li> <li>When ACM clustercurator creates the ansiblejob CR, it does not pass <code>--limit</code> variable. It is only aware of <code>extra_vars</code> variable list.</li> </ul>"},{"location":"ansible/#explicitly-enable-the-managed-serviceaccount-addon","title":"Explicitly enable the managed-serviceaccount addon","text":"<p>The managed-serviceaccount addon needs to be explicitly enabled on a given managed cluster. </p> <ul> <li>Therefore, in the usecase where customers want to in ansible jobs on newly created or imported clusters, the ansible playbook running against the new cluster should include this task. </li> </ul> <p>Here, I include the manifest need to manually enable this feature on the managed cluster.</p> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: addon.open-cluster-management.io/v1alpha1\nkind: ManagedClusterAddOn\nmetadata:\n  name: managed-serviceaccount\n  namespace: kind-cdoan-aap-integration-test-1\nspec:\n  installNamespace: open-cluster-management-agent-addon\nEOF\n</code></pre>"},{"location":"ansible/#cluster-life-cycle-post-install-ansible-job","title":"Cluster Life Cycle - post install ansible job","text":"<p>The extra variables passed to ansible job from clustercurator will look like this. The key metadata is the <code>\"{{ cluster_deployment.clusterName }}\"</code></p> <pre><code>{\n\"cluster_deployment\": {\n\"baseDomain\": \"stolostron.io\",\n\"clusterMetadata\": {\n\"adminKubeconfigSecretRef\": {\n\"name\": \"acmsre-proxy-test-2-0-8pwgn-admin-kubeconfig\"\n},\n\"adminPasswordSecretRef\": {\n\"name\": \"acmsre-proxy-test-2-0-8pwgn-admin-password\"\n},\n\"clusterID\": \"2e4acf34-39d5-4835-bafe-e4d0f132c284\",\n\"infraID\": \"acmsre-proxy-test-2-dzv2d\"\n},\n\"clusterName\": \"acmsre-proxy-test-2\",\n\"controlPlaneConfig\": {\n\"servingCertificates\": {}\n},\n\"installAttemptsLimit\": 1,\n\"installed\": true,\n\"platform\": {\n\"aws\": {\n\"credentialsSecretRef\": {\n\"name\": \"acmsre-proxy-test-2-aws-creds\"\n},\n\"region\": \"us-east-1\"\n}\n},\n...\n}\n}\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>You can get started quickly to try out these Examples.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Run this single command to deploy all the manifests into your environment.</p> <pre><code># WIP\noc apply -k ./manifests/\n</code></pre>"},{"location":"references/","title":"References","text":"<ul> <li>https://openshift.tips/</li> <li>https://github.com/stolostron/application-lifecycle-samples</li> <li>https://github.com/ch-stark/setupapplicationset-for-policies</li> </ul>"},{"location":"upgrade-downstream/","title":"Upgrade Testing","text":"<p>Setup a set of clusters to test upgrade of ACM 2.4 to ACM 2.5 from the perspective of ACM SRE. We did perform the upgrade in AOC. The number of clusters we had was &lt; 5 managed clusters. We had applications managed by argocd as well as a handful of policies. Observability was enabled.</p> <p>We did have to disable backup operator. There was another issue we had, it was documented in our word document. todo(cdoan): #link</p> <p>NOTE: Starting with release ACM 2.5, we split the original ACM into TWO products, and this means two separate <code>subscription.operators</code>. Even through you can trigger the deployment of ACM 2.5 with one <code>subscription</code>, internally, we define and apply a second <code>subscription</code> for the MCE product. MCE can be installed standalone, or it can be installed when ACM is freshly installed, or upgraded from ACM 2.4. This is seamless and somewhat transparent in a connected deployment where everything exists in the public registry.redhat.io container registry. When you are upgrading where the images are served from a private registry, the multiclusterhub operand needs to be aware of where the MCE catalogsource is located, otherwise, the upgrade will BLOCK, waiting for MCE to complete the install. TODO: Reference KCS.</p>"},{"location":"upgrade-downstream/#upgrading-from-rhacm-24-to-rhacm-25","title":"Upgrading from RHACM 2.4 to RHACM 2.5","text":"<ol> <li>6 node OCP 4.9.48 cluster, 3 master, 3 worker.</li> <li>manually install RHACM 2.4.0 GA, via kustomizaton manifests, subscription for channel release-2.4, csv: 2.4.0</li> <li>manually approve install @ 2.4.0</li> <li>deployed 10 managed clusters, 20 applications, 20 policies</li> <li>manually approve install plan @ 2.4.5</li> <li>deployed 10 managed clusters, 20 applications, 20 policies</li> <li>manually install RHACM 2.5 GA</li> <li>manually approve install plan @ 2.5</li> <li> <p>automatically approve install plan for mce 2.0</p> <pre><code>NAMESPACE                 NAME            CSV                                  APPROVAL    APPROVED\nmulticluster-engine       install-bw7b4   multicluster-engine.v2.0.2           Automatic   true\nopen-cluster-management   install-5mqxx   advanced-cluster-management.v2.4.0   Manual      true\nopen-cluster-management   install-bgnqv   advanced-cluster-management.v2.4.5   Manual      true\nopen-cluster-management   install-t5jnb   advanced-cluster-management.v2.5.2   Manual      true\n</code></pre> <pre><code>oc apply -k rhacm-operator/base-release-2.5\noc patch installplan install-t5jnb --type merge --patch '{\"spec\":{\"approved\":true}}' -n open-cluster-management\n</code></pre> <ul> <li> <p>backup was disabled in mch in 2.4</p> </li> <li> <p>mch is currently Not Running, because the local-cluster has not completed.</p> </li> </ul> <pre><code>local-cluster:\nlastTransitionTime: \"2022-09-24T13:07:31Z\"\nmessage: No conditions available\nreason: No conditions available\nstatus: Unknown\ntype: Unknown\n</code></pre> <pre><code>currentVersion: 2.4.5\ndesiredVersion: 2.5.2\nphase: Updating\n</code></pre> </li> <li> <p>When I delete all the current manifestwork, the state of all managedclusters went to <code>Unknown</code>. I dumped out the current list of manifestwork, then iterated the list and deleted each manifestwork. The expectation is that that RHACM would get back to the desired state, and the manifestwork would be recreated. As I deleted the manifestwork list, RHACM detected the change, and gradually recreated the manifestwork. Ordering is not a requirement of k8s.</p> <pre><code>Every 2.0s: oc get managedclusters ; oc get klusterletaddonconfig                       cdoan-mac: Sat Sep 24 14:59:14 2022\n\nNAME                           HUB ACCEPTED   MANAGED CLUSTER URLS                           JOINED   AVAILABLE   AGE\ncdoan-upgrade-test-1-kind      true                                                          True     Unknown     17h\nlocal-cluster                  true           https://api.cs-upgrade-24.stolostron.io:6443   True     True        18h\nmanaged-k3s-cluster-0205b515   true                                                          True     Unknown     16h\nmanaged-k3s-cluster-06d3d9e6   true                                                          True     Unknown     16h\nmanaged-k3s-cluster-07234b54   true                                                          True     Unknown     7h56m\nmanaged-k3s-cluster-1b92e12f   true                                                          True     Unknown     16h\nmanaged-k3s-cluster-1dd7cb6c   true                                                                               68s\nmanaged-k3s-cluster-21ecb4b3   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-25408850   true                                                          True     Unknown     5h59m\nmanaged-k3s-cluster-324f7846   true                                                                   Unknown     6h17m\nmanaged-k3s-cluster-39594b40   true                                                          True     Unknown     7h56m\nmanaged-k3s-cluster-3df52922   true                                                          True     Unknown     7h55m\nmanaged-k3s-cluster-3f86a180   true                                                          True     Unknown     7h55m\nmanaged-k3s-cluster-40ca86e5   true                                                          True     Unknown     7h55m\nmanaged-k3s-cluster-4aefde27   true                                                          True     Unknown     16h\nmanaged-k3s-cluster-4c92d9ff   true                                                          True     Unknown     6h4m\nmanaged-k3s-cluster-4f412f27   true                                                          True     Unknown     7h55m\nmanaged-k3s-cluster-6f333d41   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-730a70fb   true                                                          True     Unknown     16h\nmanaged-k3s-cluster-8408ce7c   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-84b7024e   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-8bda408c   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-9992d0e5   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-9c08718f   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-b4088b6d   true                                                          True     Unknown     7h55m\nmanaged-k3s-cluster-bd0890a4   true                                                          True     Unknown     7h55m\nmanaged-k3s-cluster-c71915fa   true                                                          True     Unknown     16h\nmanaged-k3s-cluster-c8f69c48   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-cd844019   true                                                          True     Unknown     16h\nmanaged-k3s-cluster-d1bae603   true                                                          True     Unknown     7h56m\nmanaged-k3s-cluster-d7f79e29   true                                                          True     Unknown     6h4m\nmanaged-k3s-cluster-dba33367   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-e05ee27b   true                                                                   Unknown     6h4m\nmanaged-k3s-cluster-e09ac848   true                                                          True     Unknown     16h\nmanaged-k3s-cluster-e5efdc53   true                                                          True     Unknown     6h4m\nmanaged-k3s-cluster-ef2230ac   true                                                          True     Unknown     7h32m\nmanaged-k3s-cluster-fd2282b7   true                                                          True     Unknown     7h56m\nmanaged-k3s-cluster-fe1b17c7   true                                                          True     Unknown     16h\nmanaged-k3s-cluster-fe8afd3e   true                                                          True     Unknown     16h\nNAME                           AGE\nmanaged-k3s-cluster-324f7846   6h17m\n</code></pre> </li> <li> <p>Now, re-iterate the list of manifestwork, and patch the finalizer and delete all those resources, that still reamin after the first pass.</p> </li> <li> <p>Checked the klusterlet namespaces on the managed cluster, they are all gone! ssh into k3s node, and running <code>kubectl get ns</code></p> <pre><code>NAME                  STATUS   AGE\ndefault               Active   17h\nkube-system           Active   17h\nkube-public           Active   17h\nkube-node-lease       Active   17h\napp-perf-sub-git-1    Active   9h\napp-perf-sub-git-2    Active   9h\napp-perf-sub-git-3    Active   9h\napp-perf-sub-git-4    Active   9h\napp-perf-sub-git-5    Active   9h\napp-perf-sub-git-6    Active   9h\n...\n</code></pre> </li> <li> <p>cleanup, deleted all managed clusters</p> </li> <li>added 10 new managed cluster and successfully imported the 10, all 10 are ready</li> <li>attempt to delete all the manifestwork, but did not force patch delete finalizer, or skip klusterlet, klusterlet-crds.</li> <li> <p>NOTE: manifestwork includes the <code>klusterlet</code> and <code>klusterlet-crds</code> manifestwork. These cannot be deleted. If this is deleted, it will remove the klusterlet from the remote managed cluster.</p> </li> <li> <p>for a given managed cluster, deleting the manifestfwork for the addons will just trigger the recreate, and have no impact to the addon status, they will continue to be available. The manifestwork for applications are not immediately recreated--perhaps at the next application reconsile interval.</p> </li> <li> <p>Turns out, its very easy to delete manifestwork. Not sure what would require deleting the finalizer on the manifestwork resource. I found that these records can be deleted without any block.</p> </li> </ol>"},{"location":"upgrade-downstream/#problems-observed","title":"Problems Observed","text":"<ol> <li>import controler pod was crashloop. One out of 2. Manually deleted the pod, and did not occur again.</li> <li><code>local-cluster</code> is in <code>Ready</code> state, but mch still shows that local-cluster from <code>mch</code> is <code>Unknown</code> state.</li> <li>3 out of n clusters is in <code>Unknown</code> state. 2 are pending import. This is likely due to the cluster itself.</li> <li>After testing deleting manifestwork, I think the reason why ALL managed cluster reached <code>Unknown</code> is because the klusterlet / klusterlet-crds manifestwork was deleted on the hub, and caused the klusterlet to be deleted on the managed cluster. The managedcluster and managedclusteraddons records should all be <code>Unknown</code>. Or, if you list them and they seem to be there, they might have been recreated, but if they were recreated, the klusterlet is probably already deleted on the remote.</li> </ol>"},{"location":"upgrade-downstream/#tools","title":"Tools","text":"description tool script to check the current status of ACM 2.5 ./hack/check-acm-status.sh script to delete all the manifestwork except klusterlet ones ./hack/chaos-delete-current-manifestwork.sh"},{"location":"upgrade-downstream/#monitoring","title":"Monitoring","text":"<p>While running this upgrade testing, its useful to monitor the resource changes using <code>watch</code>. You can run these watch commands in separate terminals.</p> <pre><code>watch 'oc get managedclusters ; oc get managedclusteraddons -A'\nwatch 'oc get manifestwork -A --sort-by=.metadata.creationTimestamp | tail -n 50; echo; echo \"count manifestwork: \" $(oc get manifestwork -A | wc -l)'\n\nstern import-con -n multicluster-engine\n</code></pre>"},{"location":"upgrade-downstream/#misc","title":"Misc","text":"<p>Example output from the <code>check-acm-status.sh</code></p> <pre><code>current acm operator phase                   :  Updating \u274c\ncurrent acm subscription currentCSV version  :  advanced-cluster-management.v2.5.2\ncurrent acm csv phase                        :  Succeeded \u2705\ncurrent mce operator phase                   :  Available \u2705\ncurrent mce subscription currentCSV version  :  multicluster-engine.v2.0.2\ncurrent mce csv phase                        :  Succeeded \u2705\ncount current managed clusters               :  8\ncount current manifestwork                   :  68\nCurrent InstallPlan State\nNAME            CSV                                  APPROVAL   APPROVED\ninstall-5mqxx   advanced-cluster-management.v2.4.0   Manual     true\ninstall-bgnqv   advanced-cluster-management.v2.4.5   Manual     true\ninstall-t5jnb   advanced-cluster-management.v2.5.2   Manual     true\nNAME            CSV                          APPROVAL    APPROVED\ninstall-bw7b4   multicluster-engine.v2.0.2   Automatic   true\n</code></pre>"},{"location":"upgrade-downstream/#automation","title":"Automation","text":"<ul> <li>It is possible to automate the creation of an upgraded environment, starting with release-2.4.</li> <li>We can leverage this same process, and upgrade to the latest release sprint drivers as well.</li> <li>Once we capture the data wither any issues were seen in upgrade, we can revert back to reinstalling the current sprint driver as a fresh install.</li> </ul>"},{"location":"upgrade-downstream/#input","title":"Input","text":"parameter description secret_name name of the secret that contains the target cluster <code>data.kubeconfig</code>, base64 encoded string. <code>data.token</code>, string. <code>data.api_endpoint</code>, string."},{"location":"argo/argo/","title":"Argo Examples","text":"<ul> <li>Argocd is a push model</li> <li>The argocd server requires access to the git repo, and pushes the manifests to the remote cluster under management.</li> <li>If the argocd server is located on a central hub, only it needs access to the git repo, and the targets do not.</li> <li>if the argcod server is deployed to each cluster it manages, then each cluster will need access to the git repo the cluster needs.</li> <li>Some customers have private clsuter deployments, where the respective git repo is not directly accessible.</li> <li> <p>ACM application component is a subscription pull model, but the channel selectors for git requires that the managed cluster will need access to the git repo as well.</p> </li> <li> <p>With playback, we have an on-prem managed cluster that resides inside the VPN.</p> </li> <li>The manage cluster can access internal gitlab, so the argo application repo is taken from here.</li> <li>This is an example of multiple clusters, and multiple git repos.</li> <li>Policy deployment based on cluster labels can select applications for <code>on-prem=true</code> vs other clusters that are running in the cloud--AWS, ARO, ROSA, etc.</li> </ul>"},{"location":"argo/argo/#application","title":"Application","text":"<p>Simple argo Applications can not be used with ACM Application placement.</p>"},{"location":"argo/argo/#applicationset","title":"ApplicationSet","text":""},{"location":"argo/argo/#external-secret-operator","title":"External Secret Operator","text":"<ol> <li>Apply the argo application for External Secrets Operator.</li> <li>ESO will be deployed to the local-cluster / in-cluster argo.</li> <li>Find the ESO application in the ACM application view. The <code>type</code> will be <code>discovered</code>.</li> <li>Argo applications cannot be placed.</li> </ol> <pre><code>oc apply -f ./docs/argo/applicationset/eso/eso.yaml\n</code></pre>"},{"location":"argo/argo/#mce","title":"MCE","text":"<p>TODO: Policy Example</p> <p>A good example argocd application is an application to deploy MCE. In ACM 2.6.0, MCE can be deployed to an attached managed cluster.</p> <p>The github repo will define the kustomization for the MCE application. We'll use External Secrets Operator (ESO) to manage the secrets, with a hashicorp Vault backend as our example. ESO supports any number of secret management platforms.</p>"},{"location":"baremetal/assisted-installer/","title":"Setup","text":"<p>Setup ODF Logicalvolume Manager Operator Policy. When a managed cluster is labeled with <code>odflvmo=true</code> the policy will be applied.</p> <pre><code>oc apply -f docs/policies/storage/odf-lvm-operator.yaml -n policies\n</code></pre> <p>or</p> <pre><code>oc apply -f https://raw.githubusercontent.com/cdoan1/examples.acm/main/docs/policies/storage/odf-lvm-operator.yaml -n policies\n</code></pre>"}]}